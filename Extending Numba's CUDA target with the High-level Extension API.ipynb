{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04ba744",
   "metadata": {},
   "source": [
    "# Extending the CUDA target with the High-Level API\n",
    "\n",
    "Quick preamble: we'll disable low-occupancy and implicit-copy warnings for this notebook, since they'll just generate irrelevant noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c9bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import config\n",
    "config.CUDA_LOW_OCCUPANCY_WARNINGS = False\n",
    "config.CUDA_WARN_ON_IMPLICIT_COPY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fb10a3",
   "metadata": {},
   "source": [
    "## What is this?\n",
    "\n",
    "Using the [High-level Extension API](https://numba.readthedocs.io/en/latest/extending/high-level.html) is the most straightforward way to extend Numba. Compared to the Low-level API:\n",
    "\n",
    "- Extension code can be written in pure Python in a lot of cases, without a lot of reference to:\n",
    "  - Numba's type inference mechanism\n",
    "  - LLVM builders and language\n",
    "- Extension code is much more \"compact\" - extension definitions can fit inside a single function, instead of having separate typing / lowering\n",
    "- There are some limitations:\n",
    "  - Defining new types and data models, and type inference rules still needs the low-level API\n",
    "\n",
    "This notebook demonstrates extending the CUDA target using the High-level API through a few examples.\n",
    "\n",
    "## Example 1: Overloading Functions\n",
    "\n",
    "Let's implement an overload of the `len()` function, for grid groups. The (moderately useless / absurd) aim is that we can write, for example:\n",
    "\n",
    "```python\n",
    "grid = cuda.cg.this_grid()\n",
    "print(\"Grid size is\", len(grid))\n",
    "```\n",
    "\n",
    "in a kernel, and get the size of the grid printed out.\n",
    "\n",
    "To use the high-level extension API with CUDA, we need the CUDA target and the `overload` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33b29bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.extending import overload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d7c6b",
   "metadata": {},
   "source": [
    "Now we'll implement our overload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deed6f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload(len, target='cuda')\n",
    "def grid_group_len(seq):\n",
    "    if isinstance(seq, cuda.types.GridGroup):\n",
    "        def len_impl(seq):\n",
    "            n = cuda.gridsize(1)\n",
    "            return n\n",
    "        return len_impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2852b1",
   "metadata": {},
   "source": [
    "Notes on the implementation:\n",
    "\n",
    "- The `@overload` decorator defines an overload.\n",
    "  - We need to specify what function is being overloaded - here it is `len()`\n",
    "  - The target here is `\"cuda\"`, but if we set it to `generic` then this overload can be used with CPU and CUDA targets.\n",
    "  - The default target is the CPU (for annoying historical reasons) so if we forget the `target` kwarg, then our overload won't work on CUDA!\n",
    "- The function gets called with argument types as its arguments\n",
    "  - In this case the function accepts one argument, `seq`.\n",
    "  - Typing is implemented by inspecting the types of the arguments.\n",
    "  - If we can successfully type this function with these arguments, then an implementation should be returned.\n",
    "  - If the typing does not succeed, we return `None` so that Numba knows it should try another overload of `len()`.\n",
    "- Returned implementations (`len_impl` in this case) should be a Python function:\n",
    "  - This function implements the logic of our overloaded function, and is compiled by Numba.\n",
    "\n",
    "Now let's use our overload in a kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4633753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid size is 1\n",
      "Grid size is 2\n",
      "Grid size is 3\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def f():\n",
    "    if cuda.grid(1) == 0:\n",
    "        print(\"Grid size is\", len(cuda.cg.this_grid()))\n",
    "        \n",
    "f[1, 1]()\n",
    "f[1, 2]()\n",
    "f[1, 3]()\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45aa894",
   "metadata": {},
   "source": [
    "... Success!\n",
    "\n",
    "## Example 2: Overloading Methods\n",
    "\n",
    "Overloading methods is similar to overloading functions, except that:\n",
    "\n",
    "- The `@overload_method()` decorator is used,\n",
    "- its first argument is the type for which the method is implemented, and\n",
    "- the second argument is the name of the method.\n",
    "\n",
    "A couple more imports we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8e4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.extending import overload_method\n",
    "from numba import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa9398",
   "metadata": {},
   "source": [
    "The CUDA target presently doesn't support the `sum()` method of NumPy arrays - we'll implement a cut-down version of it to demonstrate method overloading in the CUDA target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3af5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload_method(types.Array, 'sum', target='cuda')\n",
    "def array_sum(arr):\n",
    "    if arr.ndim != 1:\n",
    "        # Only implement 1D for this quick example\n",
    "        return None\n",
    "\n",
    "    def sum_impl(arr):\n",
    "        res = 0 \n",
    "        for i in range(len(arr)):\n",
    "            res += arr[i]\n",
    "        return res \n",
    "    return sum_impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47ab08",
   "metadata": {},
   "source": [
    "The first argument to the overload method (`arr`) is the type of the receiver.\n",
    "\n",
    "Now we can use the method in a kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7164766f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum is 10\n",
      "Sum is 45\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def f(arr):\n",
    "    print(\"Sum is\", arr.sum())\n",
    "\n",
    "\n",
    "import numpy as np    \n",
    "\n",
    "f[1, 1](np.arange(5))\n",
    "f[1, 1](np.arange(10))\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2efc8",
   "metadata": {},
   "source": [
    "## Example 3: Overloading attributes\n",
    "\n",
    "For overloading attributes, we have the `@overload_attribute` decorator - similarly to overloading functions, the decorator takes a type and an attribute name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e65cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.extending import overload_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a891427",
   "metadata": {},
   "source": [
    "Let's add an `.nbytes` attribute to arrays in CUDA kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b370a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload_attribute(types.Array, 'nbytes', target='cuda')\n",
    "def array_nbytes(arr):\n",
    "    def nbytes_impl(arr):\n",
    "        return arr.size * arr.itemsize\n",
    "    return nbytes_impl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afde43b4",
   "metadata": {},
   "source": [
    "It is immediately available for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0c3c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nbytes is 40\n",
      "Nbytes is 80\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def f(arr):\n",
    "    print(\"Nbytes is\", arr.nbytes)\n",
    "\n",
    "\n",
    "f[1, 1](np.arange(5))\n",
    "f[1, 1](np.arange(10))\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dd0b9b",
   "metadata": {},
   "source": [
    "## Example 4: Using intrinsics for lower-level control\n",
    "\n",
    "The `@overload` family of decorators provide convenient extensions with pure Python implementations - what if one needs to implement an extension using constructs not expressible in pure Python? The `@intrinsic` decorator can be used to build LLVM IR when implementing an extension. Intrinsics can be called from both `@cuda.jit`-decorated functions and overloads.\n",
    "\n",
    "For intrinsics defined for the CUDA target, we can import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c81e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.cuda.extending import intrinsic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2e300",
   "metadata": {},
   "source": [
    "We'll write an intrinsic to implement the [`clock64()` time function](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#time-function), which is not presently implemented in Numba. The prototype of this function in CUDA C is:\n",
    "\n",
    "```C\n",
    "long long int clock64();\n",
    "```\n",
    "\n",
    "NVCC translates this to a read of a PTX special register, `%clock64`. For example, generated code may look like:\n",
    "\n",
    "```asm\n",
    "mov.u64 \t%rd1, %clock64;\n",
    "```\n",
    "\n",
    "There is no way to express this only in Python, so we need an intrinsic here. Its definition could look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a45414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llvmlite import ir\n",
    "\n",
    "@intrinsic\n",
    "def cuda_clock64(typingctx):\n",
    "    sig = types.uint64()\n",
    "\n",
    "    def codegen(context, builder, sig, args):\n",
    "        function_type = ir.FunctionType(ir.IntType(64), []) \n",
    "        instruction = \"mov.u64 $0, %clock64;\"\n",
    "        clock64 = ir.InlineAsm(function_type, instruction, \"=l\",\n",
    "                               side_effect=True)\n",
    "        return builder.call(clock64, []) \n",
    "\n",
    "    return sig, codegen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d3d1f",
   "metadata": {},
   "source": [
    "Some remarks on the implementation:\n",
    "\n",
    "- The decorator itself requires no arguments.\n",
    "- The arguments to the decorated function are a typing context, and the types of the arguments to the function\n",
    "  - Here we don't need to use the typing context, but it can occasionally be useful\n",
    "      - e.g. for checking if two types will unify with `typingctx.unify_types(arg1, arg2)`\n",
    "  - We also don't have any arguments to this function, but they would appear after `typingctx` if we did.\n",
    "- The function should return a tuple of the signature and a function to generate code\n",
    "  - Or nothing, if the intrinsic couldn't be typed for these arguments\n",
    "- The code generation function is just like a normal lowering function:\n",
    "  - It gets given `context, builder, sig, args`\n",
    "  - It should build appropriate LLVM IR and return the instruction holding the result (if there is one).\n",
    "- The body of the code generation function is just a standard pattern for emitting inline PTX.\n",
    "\n",
    "Let's try this in a CUDA kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a9b2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Clock value is 6670192\n",
      "2. Clock value is 6723668\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit('void()')\n",
    "def f():\n",
    "    print(\"1. Clock value is\", cuda_clock64())\n",
    "    print(\"2. Clock value is\", cuda_clock64())\n",
    "\n",
    "\n",
    "f[1, 1]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b180034",
   "metadata": {},
   "source": [
    "It is normal to see clock values differ with every run - the second ought to be greater than the first though.\n",
    "\n",
    "## Note: mutable structures\n",
    "\n",
    "The high-level extension API also supports [implementing mutable structures](https://numba.readthedocs.io/en/latest/extending/high-level.html#implementing-mutable-structures) with `StructRef`, but this is not yet supported on the CUDA target. This is because mutable structures are heap-allocated and passed by reference, and allocation within a kernel is not yet supported on the CUDA target.\n",
    "\n",
    "Support for mutable structures / `StructRef` in CUDA is planned for a future release.\n",
    "\n",
    "## Further info:\n",
    "\n",
    "- [High-level extension API documentation](https://numba.readthedocs.io/en/latest/extending/high-level.html)\n",
    "- [An example using the High-level API and `@overload`](https://numba.readthedocs.io/en/latest/extending/overloading-guide.html) - this example focuses on a use case for the CPU target that presently doesn't map well to CUDA, but the ideas explained are generally applicable.\n",
    "- [Intrinsics in the CUDA target](https://github.com/numba/numba/blob/main/numba/cuda/intrinsics.py) - some implementations in Numba use the high-level API - see these as an example of implementing intrinsics and overloads. These include `cuda.grid()`, `cuda.gridsize()`, `syncthreads()`, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
